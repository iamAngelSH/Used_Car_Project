{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0afe0ca7-2df2-47ee-bcac-61e4f89bc67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS]\n"
     ]
    }
   ],
   "source": [
    "# IMPORT LIBRARIES\n",
    "try:\n",
    "    # PYSPARK\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark import SparkContext\n",
    "    from pyspark.sql import SQLContext\n",
    "    from pyspark.sql import DataFrame\n",
    "    import pyspark.sql.types as tp\n",
    "    import pyspark.sql.functions as F\n",
    "    \n",
    "    #Py Spark ML Libraries\n",
    "    from pyspark.ml.classification import LogisticRegression\n",
    "    from pyspark.ml.regression import LinearRegression\n",
    "    from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, StandardScaler\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
    "    \n",
    "    # OTHER LIBRARIES\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import glob\n",
    "    from functools import reduce\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    from urllib.request import urlopen\n",
    "    import datetime\n",
    "    from pathlib import Path\n",
    "    \n",
    "    print('[SUCCESS]')\n",
    "\n",
    "    #CATCH ERROR IMPORTING A LIBRARY\n",
    "except ImportError as ie:\n",
    "    raise ImportError(f'[Error importing]: {ie}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2401bf77-ef66-4e5f-83f2-22e50cc80380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESSFULLY RUNNING SPARK SESSION]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "INITIALIZING SPARK SESSION\n",
    "- NAME IS SET FOR SPARK SESSION WHEN RUNNING ON LOCAL HOST\n",
    "'''\n",
    "spark = SparkSession.builder.master('local').config(\"spark.executor.memory\", \"1g\").config(\"spark.driver.memory\", \"2g\").appName('UsedCar_Project').getOrCreate()\n",
    "print('[SUCCESSFULLY RUNNING SPARK SESSION]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2f75fc5c-bfbe-4bbf-bb1e-7fa12c97e8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------------------+-----+------------+-----+---------+--------+-----+-----+------------+-----------+-------------+--------------+\n",
      "|year| make|              model| body|transmission|state|condition|odometer|color|  mmr|sellingprice|   saledate|saledate_year|saledate_month|\n",
      "+----+-----+-------------------+-----+------------+-----+---------+--------+-----+-----+------------+-----------+-------------+--------------+\n",
      "|2015|  Kia|            Sorento|  SUV|   automatic|   ca|    Great| 16639.0|white|20500|       21500|Dec 16 2014|         2014|           Dec|\n",
      "|2015|  Kia|            Sorento|  SUV|   automatic|   ca|    Great|  9393.0|white|20800|       21500|Dec 16 2014|         2014|           Dec|\n",
      "|2014|  BMW|           3 Series|Sedan|   automatic|   ca|    Great|  1331.0| gray|31900|       30000|Jan 15 2015|         2015|           Jan|\n",
      "|2015|Volvo|                S60|Sedan|   automatic|   ca|    Great| 14282.0|white|27500|       27750|Jan 29 2015|         2015|           Jan|\n",
      "|2014|  BMW|6 Series Gran Coupe|Sedan|   automatic|   ca|    Great|  2641.0| gray|66000|       67000|Dec 18 2014|         2014|           Dec|\n",
      "+----+-----+-------------------+-----+------------+-----+---------+--------+-----+-----+------------+-----------+-------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Number of columns: 14 \n",
      "Number of Rows: 472336\n",
      "\n",
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- make: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- transmission: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- condition: string (nullable = true)\n",
      " |-- odometer: double (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- mmr: integer (nullable = true)\n",
      " |-- sellingprice: integer (nullable = true)\n",
      " |-- saledate: string (nullable = true)\n",
      " |-- saledate_year: integer (nullable = true)\n",
      " |-- saledate_month: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    '''\n",
    "    car_prices.csv is uploaded to a google bucket for public use. Since file is too large to push to GitHub for others to use from repo\n",
    "    this function will load the data from the google bucket.\n",
    "    \n",
    "    You can run this function each time and it will not download a new dataset each time since the first time you run it, it will download locally to your directory.\n",
    "    --- unless you delete it each time\n",
    "    \n",
    "    Function will check to make sure file is in the directory\n",
    "    - if it is, load it to a spark dataframe\n",
    "    - if it is not, download it, then load it to a spark dataframe\n",
    "    \n",
    "    SCHEMA:\n",
    "    - Created a schema to make sure the data types for the file being read is kept\n",
    "    \n",
    "    \n",
    "    Drop Randome Values in state columns\n",
    "    \n",
    "    \n",
    "    WARNING: TO USE THIS FUNCTION, YOU HAVE TO BE RUNNING JUPYTER NOTEBOOK ON A LINUX SERVER (USE DOCKER)\n",
    "    \n",
    "    NOTES:\n",
    "    option(\"header\",True).option(\"inferSchema\", True)\n",
    "    '''\n",
    "    \n",
    "    # CHECKS TO SEE IF FILE EXIST\n",
    "    path = Path('car_prices.csv') \n",
    "    \n",
    "    # IF FILE DOES NOT EXIST\n",
    "    if not path.is_file():\n",
    "        !wget https://storage.googleapis.com/iamangelsh-public-datasets/car_prices.csv \n",
    "    \n",
    "    \n",
    "    \n",
    "    # CREATE SCHEMA TO KEEP DATA TYPES\n",
    "    schema = tp.StructType([tp.StructField('year', tp.IntegerType(), True),\n",
    "                           tp.StructField('make', tp.StringType(), True),\n",
    "                           tp.StructField('model', tp.StringType(), True),\n",
    "                           tp.StructField('trim', tp.StringType(), True),\n",
    "                           tp.StructField('body', tp.StringType(), True),\n",
    "                           tp.StructField('transmission', tp.StringType(), True),\n",
    "                           tp.StructField('vin', tp.StringType(), True),\n",
    "                           tp.StructField('state', tp.StringType(), True),\n",
    "                           tp.StructField('condition', tp.DoubleType(), True),\n",
    "                           tp.StructField('odometer', tp.DoubleType(), True),\n",
    "                           tp.StructField('color', tp.StringType(), True),\n",
    "                           tp.StructField('interior', tp.StringType(), True),\n",
    "                           tp.StructField('seller', tp.StringType(), True),\n",
    "                           tp.StructField('mmr', tp.IntegerType(), True),\n",
    "                           tp.StructField('sellingprice', tp.IntegerType(), True),\n",
    "                           tp.StructField('saledate', tp.StringType(), True)])\n",
    "    \n",
    "    \n",
    "    # LOAD IN DATA WITH SCHEMA\n",
    "    df = spark.read.csv(\"car_prices.csv\", header = True, sep=\",\", schema=schema)\n",
    "    \n",
    "    # FILTER OUT VIN NUMBERS FROM STATE COLUMN\n",
    "    df = df.where(F.length(F.col(\"state\")) <= 2)\n",
    "    \n",
    "    # DROP ROWS THAT CONTAIN NULL VALUES\n",
    "    df = df.na.drop('any')\n",
    "    \n",
    "    # CREATE THRESHOLD FOR CONDITION COLUMN\n",
    "    df = df.withColumn(\n",
    "        'condition', \n",
    "        F.when(df.condition > 3.75, 'Great'\n",
    "        ).when((df.condition >= 2) & (df.condition <= 3.75), 'Average'\n",
    "        ).when(df.condition < 2, 'Bad'))\n",
    "    \n",
    "    # DROP COLUMNS THAT WON'T BE USED\n",
    "    cols = ('trim', 'vin', 'interior', 'seller')\n",
    "    df = df.drop(*cols)\n",
    "    \n",
    "    \n",
    "    # USE MM DD YYYY FOR SALEDATE COLUMN\n",
    "    df = df.withColumn(\n",
    "        'saledate', F.substring('saledate', 5,11)\n",
    "        ).withColumn(\n",
    "        'saledate_year', F.substring('saledate', 7,5)\n",
    "        ).withColumn(\n",
    "        'saledate_month', F.substring('saledate', 1,3))\n",
    "    \n",
    "    df = df.withColumn(\n",
    "        'saledate_year', F.col('saledate_year').cast(tp.IntegerType())\n",
    "        )\n",
    "    \n",
    "    # RETURN NEW DATAFRAME\n",
    "    return df\n",
    "\n",
    "\n",
    "# LOAD THE DATA\n",
    "df = load_data()\n",
    "\n",
    "# SHOW DATA\n",
    "df.show(5)\n",
    "\n",
    "# SHOW NUMBER OF COLUMNS AND ROWS\n",
    "print(f'Number of columns: {len(df.columns)} \\nNumber of Rows: {df.count()}')\n",
    "print()\n",
    "\n",
    "# SHOW SCHEMA - DATATYPES\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a8eac0ba-34b0-4f04-b99e-77f22e6d7cbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+\n",
      "|year|condition|sellingprice|\n",
      "+----+---------+------------+\n",
      "|2015|    Great|       21500|\n",
      "|2015|    Great|       21500|\n",
      "|2014|    Great|       30000|\n",
      "|2015|    Great|       27750|\n",
      "|2014|    Great|       67000|\n",
      "|2015|      Bad|       10900|\n",
      "|2014|  Average|       65000|\n",
      "|2014|  Average|        9800|\n",
      "|2014|    Great|       32250|\n",
      "|2014|  Average|       17500|\n",
      "|2014|    Great|       49750|\n",
      "|2015|    Great|       17700|\n",
      "|2015|  Average|       12000|\n",
      "|2015|    Great|       21500|\n",
      "|2015|  Average|       14100|\n",
      "|2014|    Great|       40000|\n",
      "|2014|      Bad|       17000|\n",
      "|2014|  Average|       67200|\n",
      "|2015|      Bad|        7200|\n",
      "|2014|  Average|       30000|\n",
      "+----+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def transform_df(df, column_names):\n",
    "    ''' SIMPLE FUNCTION THAT RETURNS \n",
    "    A NEW DF BASED ON COLUMN NAMES'''\n",
    "    return df.select(column_names)\n",
    "\n",
    "\n",
    "names = ['year', 'condition', 'sellingprice']\n",
    "df_version2 = transform_df(df, names)\n",
    "df_version2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6ddf6562-66fa-4289-9ad4-3426ad749432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+------------+-----------------+--------------------+--------------------+-------------------------+----------------------------+--------------------+\n",
      "|year|condition|sellingprice|year_indexed|condition_indexed|sellingprice_indexed|year_indexed_encoded|condition_indexed_encoded|sellingprice_indexed_encoded|            features|\n",
      "+----+---------+------------+------------+-----------------+--------------------+--------------------+-------------------------+----------------------------+--------------------+\n",
      "|2015|    Great|       21500|        12.0|              1.0|               162.0|     (26,[12],[1.0])|            (3,[1],[1.0])|          (1806,[162],[1.0])|(1835,[12,27,191]...|\n",
      "|2015|    Great|       21500|        12.0|              1.0|               162.0|     (26,[12],[1.0])|            (3,[1],[1.0])|          (1806,[162],[1.0])|(1835,[12,27,191]...|\n",
      "|2014|    Great|       30000|         2.0|              1.0|               229.0|      (26,[2],[1.0])|            (3,[1],[1.0])|          (1806,[229],[1.0])|(1835,[2,27,258],...|\n",
      "|2015|    Great|       27750|        12.0|              1.0|               344.0|     (26,[12],[1.0])|            (3,[1],[1.0])|          (1806,[344],[1.0])|(1835,[12,27,373]...|\n",
      "|2014|    Great|       67000|         2.0|              1.0|               646.0|      (26,[2],[1.0])|            (3,[1],[1.0])|          (1806,[646],[1.0])|(1835,[2,27,675],...|\n",
      "|2015|      Bad|       10900|        12.0|              2.0|                44.0|     (26,[12],[1.0])|            (3,[2],[1.0])|           (1806,[44],[1.0])|(1835,[12,28,73],...|\n",
      "|2014|  Average|       65000|         2.0|              0.0|               575.0|      (26,[2],[1.0])|            (3,[0],[1.0])|          (1806,[575],[1.0])|(1835,[2,26,604],...|\n",
      "|2014|  Average|        9800|         2.0|              0.0|                39.0|      (26,[2],[1.0])|            (3,[0],[1.0])|           (1806,[39],[1.0])|(1835,[2,26,68],[...|\n",
      "|2014|    Great|       32250|         2.0|              1.0|               405.0|      (26,[2],[1.0])|            (3,[1],[1.0])|          (1806,[405],[1.0])|(1835,[2,27,434],...|\n",
      "|2014|  Average|       17500|         2.0|              0.0|                83.0|      (26,[2],[1.0])|            (3,[0],[1.0])|           (1806,[83],[1.0])|(1835,[2,26,112],...|\n",
      "|2014|    Great|       49750|         2.0|              1.0|               642.0|      (26,[2],[1.0])|            (3,[1],[1.0])|          (1806,[642],[1.0])|(1835,[2,27,671],...|\n",
      "|2015|    Great|       17700|        12.0|              1.0|               194.0|     (26,[12],[1.0])|            (3,[1],[1.0])|          (1806,[194],[1.0])|(1835,[12,27,223]...|\n",
      "|2015|  Average|       12000|        12.0|              0.0|                 0.0|     (26,[12],[1.0])|            (3,[0],[1.0])|            (1806,[0],[1.0])|(1835,[12,26,29],...|\n",
      "|2015|    Great|       21500|        12.0|              1.0|               162.0|     (26,[12],[1.0])|            (3,[1],[1.0])|          (1806,[162],[1.0])|(1835,[12,27,191]...|\n",
      "|2015|  Average|       14100|        12.0|              0.0|               106.0|     (26,[12],[1.0])|            (3,[0],[1.0])|          (1806,[106],[1.0])|(1835,[12,26,135]...|\n",
      "|2014|    Great|       40000|         2.0|              1.0|               349.0|      (26,[2],[1.0])|            (3,[1],[1.0])|          (1806,[349],[1.0])|(1835,[2,27,378],...|\n",
      "|2014|      Bad|       17000|         2.0|              2.0|                21.0|      (26,[2],[1.0])|            (3,[2],[1.0])|           (1806,[21],[1.0])|(1835,[2,28,50],[...|\n",
      "|2014|  Average|       67200|         2.0|              0.0|              1087.0|      (26,[2],[1.0])|            (3,[0],[1.0])|         (1806,[1087],[1.0])|(1835,[2,26,1116]...|\n",
      "|2015|      Bad|        7200|        12.0|              2.0|               109.0|     (26,[12],[1.0])|            (3,[2],[1.0])|          (1806,[109],[1.0])|(1835,[12,28,138]...|\n",
      "|2014|  Average|       30000|         2.0|              0.0|               229.0|      (26,[2],[1.0])|            (3,[0],[1.0])|          (1806,[229],[1.0])|(1835,[2,26,258],...|\n",
      "+----+---------+------------+------------+-----------------+--------------------+--------------------+-------------------------+----------------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def data_processing(df, categorical_columns):\n",
    "    '''\n",
    "    FUNCTION TAKES IN 2 PARAMETERS:\n",
    "    1. Data Frame --> data frame that you are working with that you want to process\n",
    "    2. Column Names --> these are the categorical columns that will be used for processing/transformed\n",
    "    \n",
    "    Methods applied:\n",
    "    1. Indexing --> Get index of string columns\n",
    "    2. One hot encoding --> categorical values to numerical values\n",
    "    3. Assembler --> vectorizing encoded values\n",
    "    4. Pipeline --> create a pipeline do bring all these processes together\n",
    "    \n",
    "    Returns a transformed model as a dataframe\n",
    "    '''\n",
    "    \n",
    "    # 1. INDEXER\n",
    "    \n",
    "    cc = categorical_columns\n",
    "    indexers = [StringIndexer(inputCol = column, outputCol = f'{column}_indexed') for column in cc]\n",
    "    \n",
    "    # 2. One Hot Encoding\n",
    "    \n",
    "    encoders = [OneHotEncoder(dropLast = False, inputCol = idx.getOutputCol(), outputCol = f'{idx.getOutputCol()}_encoded') for idx in indexers]\n",
    "    \n",
    "    # 3. Assembler --> Vectorize encoded values\n",
    "    \n",
    "    assembler = VectorAssembler(inputCols = [encoded_val.getOutputCol() for encoded_val in encoders], outputCol = 'features')\n",
    "    \n",
    "    # 4. Pipeline\n",
    "    \n",
    "    pipeline = Pipeline(stages = indexers + encoders + [assembler])\n",
    "    \n",
    "    \n",
    "    # Return our transformed moder\n",
    "    model = pipeline.fit(df_version2)\n",
    "    \n",
    "    transformed_df = model.transform(df_version2)\n",
    "    \n",
    "    return transformed_df\n",
    "\n",
    "\n",
    "trans_df = data_processing(df_version2, names)\n",
    "trans_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c21b621a-c694-4307-b524-81839badbcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# Regression using gradient trees regressor\n",
    "####################################################\n",
    "\n",
    "# Try to predict selling price using gradient trees regressor\n",
    "import pyspark.ml.feature as ft\n",
    "\n",
    "features = ['year','condition_indexed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fff67a81-1a33-434a-9dcc-70b87ec81ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate the features together and use the ChiSqSelector to select only the top 2 most important features\n",
    "featuresCreator = ft.VectorAssembler(inputCols=[col for col in features[1:]], outputCol='features')\n",
    "selector = ft.ChiSqSelector(numTopFeatures=2, outputCol=\"selectedFeatures\", labelCol='sellingprice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7651c761-9fae-43b5-98a8-ab093146a4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to predict the selling price, we will use the gradient boosted trees generator\n",
    "import pyspark.ml.regression as reg\n",
    "regressor = reg.GBTRegressor(maxIter=15, maxDepth=3, labelCol='sellingprice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6fb64499-a897-442e-be6b-9a381307f177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+------------+\n",
      "|year|condition_indexed|sellingprice|\n",
      "+----+-----------------+------------+\n",
      "|2015|              1.0|       21500|\n",
      "|2015|              1.0|       21500|\n",
      "|2014|              1.0|       30000|\n",
      "|2015|              1.0|       27750|\n",
      "|2014|              1.0|       67000|\n",
      "|2015|              2.0|       10900|\n",
      "|2014|              0.0|       65000|\n",
      "|2014|              0.0|        9800|\n",
      "|2014|              1.0|       32250|\n",
      "|2014|              0.0|       17500|\n",
      "|2014|              1.0|       49750|\n",
      "|2015|              1.0|       17700|\n",
      "|2015|              0.0|       12000|\n",
      "|2015|              1.0|       21500|\n",
      "|2015|              0.0|       14100|\n",
      "|2014|              1.0|       40000|\n",
      "|2014|              2.0|       17000|\n",
      "|2014|              0.0|       67200|\n",
      "|2015|              2.0|        7200|\n",
      "|2014|              0.0|       30000|\n",
      "+----+-----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split data in train and test \n",
    "names = ['year', 'condition_indexed', 'sellingprice']\n",
    "df_version2 = transform_df(trans_df, names)\n",
    "df_version2.show()\n",
    "\n",
    "df_train, df_test = df_version2.randomSplit([0.7,0.3], seed=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7592bfae-1ee6-4e33-a6a7-dc76b974d894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We put it all together into a Pipeline\n",
    "pipeline = Pipeline(stages=[featuresCreator, selector, regressor])\n",
    "selling_price = pipeline.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c287ff79-5217-47c2-901e-e52321d4b139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22911336744248112\n",
      "8489.413580381832\n"
     ]
    }
   ],
   "source": [
    "# Check if mour model performs well on testing data\n",
    "import pyspark.ml.evaluation as ev\n",
    "\n",
    "evaluator = ev.RegressionEvaluator(predictionCol=\"prediction\", labelCol='sellingprice')\n",
    "\n",
    "print(evaluator.evaluate(selling_price.transform(df_test), {evaluator.metricName: 'r2'}))\n",
    "print(evaluator.evaluate(selling_price.transform(df_test), {evaluator.metricName: 'rmse'}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
